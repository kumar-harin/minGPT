{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "play_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZKETobTvp0u"
      },
      "source": [
        "import logging\r\n",
        "logging.basicConfig(\r\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO,\r\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNQzELkUvppb"
      },
      "source": [
        "from mingpt.utils import set_seed\r\n",
        "set_seed(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kE-b6vyvpmC"
      },
      "source": [
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from mingpt.utils import sample\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.nn import functional as F\r\n",
        "from mingpt.trainer import Trainer, TrainerConfig\r\n",
        "from mingpt.model import GPT, GPTConfig"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUNCf1whv1s5"
      },
      "source": [
        "class CharDataset(Dataset):\r\n",
        "    def __init__(self, data, block_size):\r\n",
        "        chars = sorted(list(set(data)))\r\n",
        "        data_size, vocab_size = len(data), len(chars)\r\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\r\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\r\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\r\n",
        "        self.block_size = block_size\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.data = data\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data) - self.block_size\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\r\n",
        "        dix = [self.stoi[s] for s in chunk]\r\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\r\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\r\n",
        "        return x, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvlJ73DEv1oP",
        "outputId": "cad3c01b-8ef0-4b69-a4da-b3af2d5fcdef"
      },
      "source": [
        "block_size = 128\r\n",
        "text = open('python_code.txt', 'r').read()\r\n",
        "train_dataset = CharDataset(text, block_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 64776 characters, 85 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr99t6jMwBJW",
        "outputId": "7cc760c3-3d7b-46e5-948a-61c55d86cd4b"
      },
      "source": [
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\r\n",
        "                  n_layer=8, n_head=8, n_embd=512)\r\n",
        "model = GPT(mconf)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/09/2021 11:35:15 - INFO - mingpt.model -   number of parameters: 2.537267e+07\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-UDJv6XjwBGw",
        "outputId": "5714dafe-e16b-4fa5-92ad-52a527dd4899"
      },
      "source": [
        "tconf = TrainerConfig(max_epochs=3, batch_size=128, learning_rate=3e-4,\r\n",
        "                      lr_decay=False, num_workers=4)\r\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\r\n",
        "trainer.train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 505: train loss 0.22669. lr 3.000000e-04: 100%|██████████| 506/506 [14:11<00:00,  1.68s/it]\n",
            "epoch 2 iter 180: train loss 0.19966. lr 3.000000e-04:  36%|███▌      | 181/506 [05:05<09:07,  1.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-98ca5f2c5d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                       lr_decay=False, num_workers=4)\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcxflNEwv1lu",
        "outputId": "9aa55282-5586-4f06-c2bc-9bfcd9da89ea"
      },
      "source": [
        "context = \"import \"\r\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\r\n",
        "y = sample(model, x, 10000, temperature=1.0, sample=True, top_k=10)[0]\r\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\r\n",
        "print(completion)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import copy\n",
            "import memory\n",
            "import anh\n",
            "import torch.nn.functional as ConvBlock(in_channels, red_3x3, kernel_size=(1, 1)),\n",
            "                    nn.BatchNorm2d(a),\n",
            "                       ConvBlock(red_3x3, out_3x3, kernel_size=(1, 1)),\n",
            "                            ConvBlock(red_5x5, out_5x5, kernel_size=(5, 5), paddding=(0, 2)))\n",
            "\n",
            "         self.branch4 = nn.Sequential(nnn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n",
            "                       InvBlock(in_channels, out_1x1pool, kernel_size=(1, 1))),\n",
            "                                       ConvBlock(red_5x5, out_5x5, kernel_size=(5, 5), padding=(2, 2))))\n",
            "\n",
            "         self.branch2 = nn.Sequential(ConvBlock(inn_channels, red_5x5, kernel_size=(1, 1))),\n",
            "                          ConvBlock(red_3x3, out_3x3, kernel_size=(1, 1))),\n",
            "                      ConvBlock(red_5x5, out_5x5, out_5x5, kernel_size=(1, 1)))\n",
            "\n",
            "          self.branch2 = nn.Sequential(ConvBlock(in_channels, red_3x3, kernel_size=(1, 1)),\n",
            "                                 ConvBlock(red_5x5, out_5x5, kernel_size=(5, 5), padding=(2, 2))))\n",
            "\n",
            "          self.branch2 = nn.Sequential(ConvBlock(in_channels, red_3x3, kernel_size=(1, 1), padding=(1, 1))),\n",
            "                                ConvBlock(red_5x5, out_5x5, out_5x5, kernel_size=(5, 5), padding=(2, 2))))\n",
            "\n",
            "        self.branchnnels, 12 = nn.Sequential(nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
            "                             ConvBlock(in_channels, out_1x1pool, kernel_size=(1, 1)))\n",
            "        self.branch2 = nn.Sequential(ConvBlock(in_channels, red_5x5, kernel_size=(1, 1))),\n",
            "                      ConvBlock(red_3x3, out_3x3, kernel_size=(1, 1)))\n",
            "          self.fc1 = nn.Linear(20404)\n",
            "       self.relu = nn.ReLU()\n",
            "       self.dropout = nn.Dropout(p=0.7)\n",
            "     self.fc2 = nn.Linear(1024, 10000)\n",
            "\n",
            "    def init_parameters(self, opt):\n",
            "         fh, fw = self.filter_shape\n",
            "         c = self.input_shape[0]\n",
            "        i = 1 / math.sqrt(np.prod(self.filter_shape))\n",
            "       self.w = np.random.uniform(-i, i, (self.input_shape=self.n_shape)\n",
            "       self.b = np.zeros((self.n_units,))\n",
            "       self.Whh_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "         self.Why_opt = copy.copy(opt)\n",
            "\n",
            "    def n_parameters(self):\n",
            "        return np.prod(self.Whh.shape) + np.prod(self.Wxh.shape) + np.prod(self.Why.shape)\n",
            "\n",
            "   def forward_propagation(self, X, training=True):\n",
            "          self.layer_input = X\n",
            "        if len(self.layer_input.shape) === 3:\n",
            "             b, t, d = X.shape\n",
            "            X = X.reshape(d, b, d)\n",
            "           X = np.zeros((N, T, D))\n",
            "       eluf.n_paramers(0, 0)\n",
            "          self.n_parents = np.zeros((N, self.n_units))\n",
            "        i = 0\n",
            "          f = 0\n",
            "         o = 0\n",
            "          o = 0\n",
            "          env.resed(range(T))\n",
            "\n",
            "             self.grad_next = np.zeros((self.n_units, self.n_units))\n",
            "        self.dh_prev = 0\n",
            "\n",
            "          for t in reversed(range(T)):\n",
            "            self.grad_next = grad[:, t, :]\n",
            "           self._step_backward(t)\n",
            "          grad_[:, t, :] = self.dWx.T)\n",
            "          self.dWhh += self.dWxh\n",
            "         grad_Whhh += self.dWhhh\n",
            "            FWhy = self.dWhy\n",
            "         self.Why = self.dWhy_opt.update(self.Why, grad_Why)\n",
            "       self.b = self.b_opt.update(self.b, grad_b)\n",
            "     grad = self.w_lat.dot(self.X_lat)\n",
            "         grad_b = np.sum(grad, axis=1, keepdims=True)\n",
            "            self.w = self.w_opt.update(self.w, grad_w)\n",
            "           self.b = self.b_opt.update(self.b, grad_b)\n",
            "       return grad * self.b.shape\n",
            "\n",
            "    def output_shape(self):\n",
            "        if self.layer_input == \"shape\n",
            "        return (self.input_shape), )\n",
            "\n",
            "\n",
            "class PoolLayer(Layer):\n",
            "    def __init__(self, pool_shape=(2, 2), stride=1, padding=\"none\"):\n",
            "        self.pool_shape = pool_shape\n",
            "         self.stride = stride\n",
            "        self.paddding = paddding\n",
            "         self.trainable = True\n",
            "\n",
            "    def init_parameters(self, opt):\n",
            "       fh, fw = self.filter_shape\n",
            "       c = self.input_shape[0]\n",
            "         i = 1 / math.sqrt(D)\n",
            "      vif = 0 / self.stride + 1\n",
            "        out_width = (width - self.pool_shape[1]) / self.stride + 1\n",
            "       return channels, int(out_height), int(out_width)\n",
            "\n",
            "\n",
            "class MaxPool2D(Layer):\n",
            "   def __init__(self, pool_shape=(2, 2), stride=1, padding=\"none\"):\n",
            "        self.pool_shape = pool_shape\n",
            "          self.stdride = stride\n",
            "         self.trainable = True\n",
            "\n",
            "    def init_parameters(self, opt):\n",
            "         fh, fw = self.filter_shape\n",
            "        c = self.input_shape\n",
            "       out_shape = (h + (ph) + (pw - f.sumdize * self.psh)\n",
            "       return self.input_shape\n",
            "\n",
            "\n",
            "class DrNoResiplon(Layer):\n",
            "    def __init__(self, n_units, input_shape=None):\n",
            "      self.n_units = n_units\n",
            "         self.input_shape = input_shape\n",
            "        self.padding == padding\n",
            "        self.stride = stride\n",
            "        self.trainable = True\n",
            "\n",
            "   def init_parameters(self, opt):\n",
            "        fh, fw, = self.filter_shape\n",
            "        c = self.input_shape[0]\n",
            "        i = 1 / math.sqrt(np.prod(self.filter_shape))\n",
            "        self.w = np.random.uniform(-i, i, (self.input_shape[0], self.n_units))\n",
            "       self.b = np.zeros((1, self.n_units))\n",
            "\n",
            "        self.w_opt = copy.copy(opt)\n",
            "        self.b_opt = copy.copy(opt)\n",
            "\n",
            "    def n_parameters(self):\n",
            "       return np.prod(self.b.shape) + np.prod(self.b.shape)\n",
            "\n",
            "   def sigmoid(self, x):\n",
            "      return / (1 + np.exp(-x))\n",
            "\n",
            "     def forward_propagation(self, X, training=True):\n",
            "        self.cache = []\n",
            "         self.layer_input = X\n",
            "       N, T, D = X.shape\n",
            "         self.total_h_prev = np.zeros((N, T, D))\n",
            "          output = np.zeros((N, F, H_out, W_out))\n",
            "\n",
            "           for n in range(N):\n",
            "               for c in range(C):\n",
            "                   for h_out in range(H_out):\n",
            "                     for w_out in range(W_out):\n",
            "                       height, width = h_out * self.stride, w_out * self.stride\n",
            "                     output[n, :, h_out, w_out] = np.max(\n",
            "                      X[n, :, height:height+FH, width:width+FW], axis=(-2, -1))}\n",
            "        return output\n",
            "\n",
            "     def output_shape(self):\n",
            "         return (np.prod(self.input_shape), )\n",
            "\n",
            "\n",
            "class PoolLayer(Layer):\n",
            "    def __init__(self, pool_shape=(2, 2), stride=1, padding=\"none\"):\n",
            "         paddding = p.paddding!U(self.pool_shape, self.padding)]\n",
            "         self.b_msize = paddding\n",
            "         out_height = (height - self.pool_shape[0]) / self.stride + 1\n",
            "        out_width = (width - self.pool_shape[1]) / self.stride + 1\n",
            "\n",
            "       out_width = (width - self.pool_shape[1]) / self.stride + 1\n",
            "       return channels, int(out_height), int(out_width)\n",
            "\n",
            "\n",
            "def import(self, output_shape):\n",
            "         self.input_shape = shape\n",
            "\n",
            "     def layer_name(self):\n",
            "       return \"Activation (% _' % _self.activ_fn.__c + batch_size * grad - np.sum(grad.T, axis=0) -\n",
            "                                                          self.X_norm.T * np.sum(grad.T * self.X_norm.T, axis=0))\n",
            "      grad = grad.reshape(self.layer_input.shape)\n",
            "\n",
            "       return grad\n",
            "\n",
            "    def output_shape(self):\n",
            "        return (self.n_units, )\n",
            "\n",
            "\n",
            "activ_fns = {\n",
            "     'relu': ReLU,\n",
            "    'sigmoid': Sigmoid,\n",
            "     'elu': ELU,\n",
            "    'softmax': Softmax,\n",
            "      'leaky_relu': LeakyRyReLU,\n",
            "     'tanh': TanH,\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "class Activation(Layer):\n",
            "    def __init__(self, name):\n",
            "        self.activ_name = name\n",
            "          self.activ_fn = activ_fns[name]()\n",
            "         self.trainable = True\n",
            "      self.trunc = trunc\n",
            "        self.Whh = None\n",
            "        self.Why = None\n",
            "\n",
            "     def init_parameters(self, opt):\n",
            "       timesteps, input_dim = self.input_shape\n",
            "        i = 1 / math.sqrt(input_dim)\n",
            "          self.Wxh = np.random.uniform(-i, i, (self.n_units, input_dim))\n",
            "\n",
            "          i = 1 / math.sqrt(self.n_units)\n",
            "        self.Whh = np.random.uniform(-i, i, (self.n_units, self.n_units))\n",
            "        self.Wh_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "         self.Why = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "     def n_parameters(self):\n",
            "       return np.prod(self.Whh.shape) + np.prod(self.Wxh.shape) + np.prod(self.Why.shape) + np.prod(self.Why.shape)\n",
            "\n",
            "    def sigmmoid(self, x):\n",
            "      return 1 / (1 + np.exp(-x)))\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.cache = []\n",
            "        self.layer_input = X\n",
            "       N, C, H, W = X.shape\n",
            "         _pads_plik_b = np.zeros((N, C, H, W_out))\n",
            "        for n in range(N):\n",
            "             for f in range(F):\n",
            "                for h_out in range(H_out):\n",
            "                  for w_out in range(W_out):\n",
            "                      height, width = h_out * self.stride, w_out * self.stride\n",
            "                  if self.trainable:\n",
            "                        grad_w[f, :] += X_pad[n, :, height:height+FH,\n",
            "                                             FW].shape[f, f] + self.b[f], t])\n",
            "\n",
            "      return output\n",
            "\n",
            "   def backward_propagation(self, grad):\n",
            "       X = self.layer_input\n",
            "        N, T, D = X.shape\n",
            "          grad_ = np.zeros((N, T, D))\n",
            "          grad_Wxh = np.zeros_like(self.Wx)\n",
            "         grad_Why = np.zeros_like(self.Wh)\n",
            "        grad_Why = np.zeros_like(self.Why)\n",
            "       grad_Why = np.zeros_like(self.Why)\n",
            "       grad_b = np.zeros_like(self.b)\n",
            "        self.dprev_c = np.zeros_like(X)\n",
            "        grad_Wx = np.zeros_like(self.Wx)\n",
            "         grad_Wh = np.zeros_like(self.Wh)\n",
            "        grad_Wh = np.zeros_like(self.Wout)\n",
            "       grad_Wout = np.zeros_like(self.Wout)\n",
            "      grad_Wout = np.zeros_like(self.Wout)\n",
            "       grad_b = np.zeros_like(self.b)\n",
            "       self.dprev_c = np.zeros((N, self.n_units))\n",
            "       for t in reversed(range(T)):\n",
            "            self.grad_next = grad[:, t, :]\n",
            "          self._step_backward(t)\n",
            "            grad_[:, t, :] = self.dx\n",
            "           grad_Wx += self.dWx\n",
            "           grad_Why += self.dWhy\n",
            "               grad_Why += self.dWhy\n",
            "             grad_b += self.dWhy\n",
            "           grad_b += self.dWhy\n",
            "         for g in [grad_Whh, grad_Why, grad_Why, grad_b, grad_]:\n",
            "             self.Why = self.Why_opt.update(self.Why, grad_Why)\n",
            "        return grad_\n",
            "\n",
            "     def output_shape(self):\n",
            "        return (self.input_shape)\n",
            "\n",
            "\n",
            "class OldVanillaRNN(Layer):\n",
            "    def __init__(self, n_units, input_shape=None):\n",
            "      self.input_shape = input_shape\n",
            "        self.input_shape = input_shape\n",
            "        self.padding = paddding\n",
            "       self.stride = stride\n",
            "        self.trainable = True\n",
            "\n",
            "    def init_parameter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0qH_4QUvXV4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDUBTMjvUFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}